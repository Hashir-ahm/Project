{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1186e67c-976a-46a8-a92c-6e267b0e59ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['iso3', 'year', 'country_x', 'iso2_x', 'iso.numeric_x', 'g.whoregion_x',\n",
       "       'new.sp', 'new.sn', 'new.su', 'new.ep',\n",
       "       ...\n",
       "       'c.mdr.tsr', 'xdr.coh', 'xdr.cur', 'xdr.cmplt', 'xdr.succ', 'xdr.died',\n",
       "       'xdr.fail', 'xdr.lost', 'c.xdr.neval', 'c.xdr.tsr'],\n",
       "      dtype='object', length=386)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the files after re-upload\n",
    "\n",
    "file1 = pd.read_csv('tb_2022-09-30.csv')\n",
    "file2 = pd.read_csv('tbhiv_2022-09-27.csv')\n",
    "file3 = pd.read_csv('tx_2022-08-29.csv')\n",
    "\n",
    "# Check if 'iso3' and 'year' columns exist in all files\n",
    "common_columns = ['iso3', 'year']\n",
    "\n",
    "# Merge the files on 'iso3' and 'year'\n",
    "merged_df = file1.merge(file2, on=common_columns, how='outer').merge(file3, on=common_columns, how='outer')\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6251d43-81dd-4762-991a-5cb8dff15348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azhaff\\AppData\\Local\\Temp\\ipykernel_23324\\2651369342.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  cleaned_df[col].fillna(cleaned_df[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Azhaff\\AppData\\Local\\Temp\\ipykernel_23324\\2651369342.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  cleaned_df[col].fillna(cleaned_df[col].median(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'final_cleaned_data_v2.csv'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned file that you want to process\n",
    "cleaned_df = pd.read_csv('/mnt/data/cleaned_merged_data.csv')\n",
    "\n",
    "# Function to remove columns with more than 50% NULL values\n",
    "def drop_columns_with_nulls(df, threshold=0.5):\n",
    "    # Calculate the percentage of NULL values in each column\n",
    "    null_percentage = df.isnull().mean()\n",
    "    # Drop columns where the NULL percentage is greater than the threshold\n",
    "    return df.loc[:, null_percentage <= threshold]\n",
    "\n",
    "# Apply the function to drop columns with more than 50% NULL values\n",
    "cleaned_df = drop_columns_with_nulls(cleaned_df)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['iso2_y', 'g.whoregion']\n",
    "cleaned_df = cleaned_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Fill remaining NULL values in the dataset for numerical columns with median and categorical columns with mode\n",
    "for col in cleaned_df.columns:\n",
    "    if cleaned_df[col].dtype in ['float64', 'int64']:  # numerical columns\n",
    "        cleaned_df[col].fillna(cleaned_df[col].median(), inplace=True)\n",
    "    elif cleaned_df[col].dtype == 'object':  # categorical columns\n",
    "        cleaned_df[col].fillna(cleaned_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop duplicate columns (if any exist after merging)\n",
    "cleaned_df = cleaned_df.loc[:, ~cleaned_df.columns.duplicated()]\n",
    "\n",
    "# Save the final cleaned dataframe\n",
    "output_file_path = 'final_cleaned_data_v2.csv'\n",
    "cleaned_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Provide the path for downloading the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1393df7-7b49-458b-8553-660cfeb9a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azhaff\\AppData\\Local\\Temp\\ipykernel_23324\\938176511.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_cleaned_df[col].fillna(final_cleaned_df[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Azhaff\\AppData\\Local\\Temp\\ipykernel_23324\\938176511.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_cleaned_df[col].fillna(final_cleaned_df[col].median(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'preprocessed_data_for_visualization.csv'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the final dataset\n",
    "final_cleaned_df = pd.read_csv('/mnt/data/final_cleaned_data_v2.csv')\n",
    "\n",
    "# Function to remove columns with more than 50% NULL values\n",
    "def drop_columns_with_nulls(df, threshold=0.5):\n",
    "    null_percentage = df.isnull().mean()\n",
    "    return df.loc[:, null_percentage <= threshold]\n",
    "\n",
    "# Apply the function to drop columns with more than 50% NULL values\n",
    "final_cleaned_df = drop_columns_with_nulls(final_cleaned_df)\n",
    "\n",
    "# Drop unnecessary columns (e.g., iso2_y, g.whoregion, and any duplicated columns)\n",
    "columns_to_drop = ['iso2_y', 'g.whoregion']\n",
    "final_cleaned_df = final_cleaned_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Fill remaining NULL values for numerical columns with median, and categorical columns with mode\n",
    "for col in final_cleaned_df.columns:\n",
    "    if final_cleaned_df[col].dtype in ['float64', 'int64']:  # Numerical columns\n",
    "        final_cleaned_df[col].fillna(final_cleaned_df[col].median(), inplace=True)\n",
    "    elif final_cleaned_df[col].dtype == 'object':  # Categorical columns\n",
    "        final_cleaned_df[col].fillna(final_cleaned_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop duplicate columns, if any\n",
    "final_cleaned_df = final_cleaned_df.loc[:, ~final_cleaned_df.columns.duplicated()]\n",
    "\n",
    "# Save the final preprocessed dataset to a new file\n",
    "output_file_path = 'preprocessed_data_for_visualization.csv'\n",
    "final_cleaned_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Provide the path for downloading the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff704d0b-7ad1-4ad7-9df6-acd3eb024190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  iso3  year country_x iso2_x  iso.numeric_x g.whoregion_x  new.ep  \\\n",
       " 0  ABW  1980     Aruba     AW          533.0           AMR   365.0   \n",
       " 1  ABW  1981     Aruba     AW          533.0           AMR   365.0   \n",
       " 2  ABW  1982     Aruba     AW          533.0           AMR   365.0   \n",
       " 3  ABW  1983     Aruba     AW          533.0           AMR   365.0   \n",
       " 4  ABW  1984     Aruba     AW          533.0           AMR   365.0   \n",
       " \n",
       "    tot.newrel  c.newunk  c.newinc_x  ...  c.new.tsr_x   ch      conf  \\\n",
       " 0      2118.5    2231.5      2158.0  ...         81.0  0.0  0.702607   \n",
       " 1      2118.5    2231.5      2158.0  ...         81.0  0.0  0.702607   \n",
       " 2      2118.5    2231.5      2158.0  ...         81.0  0.0  0.702607   \n",
       " 3      2118.5    2231.5      2158.0  ...         81.0  0.0  0.702607   \n",
       " 4      2118.5    2231.5      2158.0  ...         81.0  0.0  0.702607   \n",
       " \n",
       "          ep  g.whoregion_y  c.newinc_y  c.notified_y    country_y  \\\n",
       " 0  0.090036            EUR      2448.0        2535.5  Afghanistan   \n",
       " 1  0.090036            EUR      2448.0        2535.5  Afghanistan   \n",
       " 2  0.090036            EUR      2448.0        2535.5  Afghanistan   \n",
       " 3  0.090036            EUR      2448.0        2535.5  Afghanistan   \n",
       " 4  0.090036            EUR      2448.0        2535.5  Afghanistan   \n",
       " \n",
       "    iso.numeric_y  c.new.tsr_y  \n",
       " 0          430.0         81.0  \n",
       " 1          430.0         81.0  \n",
       " 2          430.0         81.0  \n",
       " 3          430.0         81.0  \n",
       " 4          430.0         81.0  \n",
       " \n",
       " [5 rows x 47 columns],\n",
       "          Country ISO-ALPHA-3 ISO-ALPHA-2  IOC FIFA   Latitude  Longitude  \\\n",
       " 0          Aruba         ABW          AW  ARU  ARU  12.521110 -69.968338   \n",
       " 1    Afghanistan         AFG          AF  AFG  AFG  33.939110  67.709953   \n",
       " 2         Angola         AGO          AO  ANG  ANG -11.202692  17.873887   \n",
       " 3       Anguilla         AIA          AI  NaN  AIA  18.220554 -63.068615   \n",
       " 4  Åland Islands         ALA         NaN  NaN  NaN  60.250716  20.374146   \n",
       " \n",
       "    ISO-Name  Historical WikiData_ID  WikiData_Latitude  WikiData_Longitude  \\\n",
       " 0         1           0      Q21203              12.51              -69.97   \n",
       " 1         1           0        Q889              33.00               66.00   \n",
       " 2         1           0        Q916             -12.35               17.35   \n",
       " 3         1           0      Q25228              18.23              -63.05   \n",
       " 4         1           0       Q5689              60.25               20.00   \n",
       " \n",
       "   WikiData_Label                               WikiData_Description  \n",
       " 0          Aruba  island country in the Caribbean, part of the K...  \n",
       " 1    Afghanistan  sovereign state situated at the confluence of ...  \n",
       " 2         Angola                          sovereign state in Africa  \n",
       " 3       Anguilla        British Overseas Territory in the Caribbean  \n",
       " 4          Åland                       autonomous region of Finland  )"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two uploaded files\n",
    "preprocessed_data_path = 'preprocessed_data_for_visualization.csv'\n",
    "longitude_latitude_path = 'longitude-latitude.csv'\n",
    "\n",
    "preprocessed_data = pd.read_csv(preprocessed_data_path)\n",
    "longitude_latitude = pd.read_csv(longitude_latitude_path)\n",
    "\n",
    "# Check the first few rows of each dataset to understand their structure\n",
    "preprocessed_data.head(), longitude_latitude.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e32d0ca-0f6b-4a8e-9717-facac70ac3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Displaying the updated data\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerged Data with Latitude and Longitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mmerged_data)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Checking the first few rows to ensure the merge was successful\u001b[39;00m\n\u001b[0;32m     12\u001b[0m merged_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "# Merging the two datasets on the country names\n",
    "merged_data = pd.merge(preprocessed_data, longitude_latitude[['Country', 'Latitude', 'Longitude']], \n",
    "                       left_on='country_x', right_on='Country', how='left')\n",
    "\n",
    "# Dropping the unnecessary columns after the merge\n",
    "merged_data = merged_data.drop(columns=['Country'])\n",
    "\n",
    "# Displaying the updated data\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Merged Data with Latitude and Longitude\", dataframe=merged_data)\n",
    "\n",
    "# Checking the first few rows to ensure the merge was successful\n",
    "merged_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41448ec1-583f-45bb-9d7f-fdf35798c455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_data.json'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Merged_Data_with_Latitude_and_Longitude.csv')\n",
    "\n",
    "# Creating nodes (unique countries involved in relationships)\n",
    "nodes = pd.DataFrame({'id': pd.concat([df['country_x'], df['country_y']]).unique()})\n",
    "\n",
    "# Creating links (relationships between countries based on country_x and country_y columns)\n",
    "links = pd.DataFrame({\n",
    "    'source': df['country_x'],\n",
    "    'target': df['country_y']\n",
    "})\n",
    "\n",
    "# Converting nodes and links to the required JSON format for D3.js\n",
    "graph_data = {\n",
    "    'nodes': nodes.to_dict(orient='records'),\n",
    "    'links': links.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "# Save the data as a JSON file for use in the D3.js visualization\n",
    "output_json_path = 'graph_data.json'\n",
    "with open(output_json_path, 'w') as f:\n",
    "    json.dump(graph_data, f)\n",
    "\n",
    "# Provide the path for downloading the JSON file\n",
    "output_json_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9d0602a-4d1c-481d-9dbd-3166adb97dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'iso3', 'year', 'country_x', 'iso2_x', 'iso.numeric_x',\n",
      "       'g.whoregion_x', 'new.ep', 'tot.newrel', 'c.newunk', 'c.newinc_x',\n",
      "       'c.ret', 'c.notified_x', 'c.new.014', 'e.pop.m04', 'e.pop.m514',\n",
      "       'e.pop.m014', 'e.pop.m1524', 'e.pop.m2534', 'e.pop.m3544',\n",
      "       'e.pop.m4554', 'e.pop.m5564', 'e.pop.m65', 'e.pop.m15plus', 'e.pop.f04',\n",
      "       'e.pop.f514', 'e.pop.f014', 'e.pop.f1524', 'e.pop.f2534', 'e.pop.f3544',\n",
      "       'e.pop.f4554', 'e.pop.f5564', 'e.pop.f65', 'e.pop.f15plus',\n",
      "       'e.pop.15plus', 'e.pop.num', 'pop', 'newinc_x', 'c.new.tsr_x', 'ch',\n",
      "       'conf', 'ep', 'g.whoregion_y', 'c.newinc_y', 'c.notified_y',\n",
      "       'country_y', 'iso.numeric_y', 'c.new.tsr_y', 'Latitude', 'Longitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Merged_Data_with_Latitude_and_Longitude.csv')\n",
    "\n",
    "# Print the column names to verify the correct ones for latitude and longitude\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f0f192e-772d-48f5-9331-4680c37cff93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_data_for_mapchart.json'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Merged_Data_with_Latitude_and_Longitude.csv')\n",
    "\n",
    "# Check the column names and adjust the names accordingly (e.g., 'lat_x', 'lon_x', etc.)\n",
    "nodes = pd.DataFrame({\n",
    "    'id': pd.concat([df['country_x'], df['country_y']]).unique()\n",
    "})\n",
    "\n",
    "# Add latitude and longitude for each country (adjust column names if necessary)\n",
    "nodes['lat'] = nodes['id'].map(df.set_index('country_x')['Latitude'].to_dict())  # Replace 'Latitude' with the correct column name\n",
    "nodes['lon'] = nodes['id'].map(df.set_index('country_x')['Longitude'].to_dict())  # Replace 'Longitude' with the correct column name\n",
    "\n",
    "# Remove rows with NaN latitude or longitude values\n",
    "nodes = nodes.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "# Creating links (relationships between countries based on country_x and country_y columns)\n",
    "links = pd.DataFrame({\n",
    "    'source': df['country_x'],\n",
    "    'target': df['country_y']\n",
    "})\n",
    "\n",
    "# Convert nodes and links to the required JSON format for D3.js\n",
    "graph_data = {\n",
    "    'nodes': nodes.to_dict(orient='records'),\n",
    "    'links': links.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "# Save the data as a JSON file for use in the D3.js visualization\n",
    "output_json_path = 'graph_data_for_mapchart.json'\n",
    "with open(output_json_path, 'w') as f:\n",
    "    json.dump(graph_data, f)\n",
    "\n",
    "# Provide the path for downloading the JSON file\n",
    "output_json_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bcb1cebc-16af-4afe-93c3-c26a879ee350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'iso3', 'year', 'country_x', 'iso2_x', 'iso.numeric_x',\n",
       "       'g.whoregion_x', 'new.ep', 'tot.newrel', 'c.newunk', 'c.newinc_x',\n",
       "       'c.ret', 'c.notified_x', 'c.new.014', 'e.pop.m04', 'e.pop.m514',\n",
       "       'e.pop.m014', 'e.pop.m1524', 'e.pop.m2534', 'e.pop.m3544',\n",
       "       'e.pop.m4554', 'e.pop.m5564', 'e.pop.m65', 'e.pop.m15plus', 'e.pop.f04',\n",
       "       'e.pop.f514', 'e.pop.f014', 'e.pop.f1524', 'e.pop.f2534', 'e.pop.f3544',\n",
       "       'e.pop.f4554', 'e.pop.f5564', 'e.pop.f65', 'e.pop.f15plus',\n",
       "       'e.pop.15plus', 'e.pop.num', 'pop', 'newinc_x', 'c.new.tsr_x', 'ch',\n",
       "       'conf', 'ep', 'g.whoregion_y', 'c.newinc_y', 'c.notified_y',\n",
       "       'country_y', 'iso.numeric_y', 'c.new.tsr_y', 'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the newly uploaded CSV data\n",
    "file_path = 'Merged_Data_with_Latitude_and_Longitude.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the columns to understand the structure of the file\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fe189dd1-6c27-4003-83eb-9ba8abc7a37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bubble_chart_data.json'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the necessary columns for the bubble chart\n",
    "df_processed = df[['country_y', 'g.whoregion_y', 'new.ep', 'pop', 'year']]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "df_processed = df_processed.rename(columns={\n",
    "    'country_y': 'Country',\n",
    "    'g.whoregion_y': 'Region',\n",
    "    'new.ep': 'Life Expectancy',\n",
    "    'pop': 'Population',\n",
    "    'year': 'Year'\n",
    "})\n",
    "\n",
    "# Create a new column for Bubble Size based on the population\n",
    "df_processed['Bubble Size'] = df_processed['Population'] / 1000000  # Scale the bubble size\n",
    "\n",
    "# Convert the DataFrame to a dictionary format suitable for JSON\n",
    "json_data = df_processed.to_dict(orient='records')\n",
    "\n",
    "# Save to a JSON file\n",
    "json_file_path = 'bubble_chart_data.json'\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "# Provide the link to download the generated JSON file\n",
    "json_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "efe361be-08bf-4917-9167-a26c9bd63db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>iso3</th>\n",
       "      <th>year</th>\n",
       "      <th>country_x</th>\n",
       "      <th>iso2_x</th>\n",
       "      <th>iso.numeric_x</th>\n",
       "      <th>g.whoregion_x</th>\n",
       "      <th>new.ep</th>\n",
       "      <th>tot.newrel</th>\n",
       "      <th>c.newunk</th>\n",
       "      <th>...</th>\n",
       "      <th>conf</th>\n",
       "      <th>ep</th>\n",
       "      <th>g.whoregion_y</th>\n",
       "      <th>c.newinc_y</th>\n",
       "      <th>c.notified_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>iso.numeric_y</th>\n",
       "      <th>c.new.tsr_y</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1980</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1981</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1982</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1983</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1984</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 iso3  year country_x iso2_x  iso.numeric_x g.whoregion_x  \\\n",
       "0           0  ABW  1980     Aruba     AW          533.0           AMR   \n",
       "1           1  ABW  1981     Aruba     AW          533.0           AMR   \n",
       "2           2  ABW  1982     Aruba     AW          533.0           AMR   \n",
       "3           3  ABW  1983     Aruba     AW          533.0           AMR   \n",
       "4           4  ABW  1984     Aruba     AW          533.0           AMR   \n",
       "\n",
       "   new.ep  tot.newrel  c.newunk  ...      conf        ep  g.whoregion_y  \\\n",
       "0   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "1   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "2   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "3   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "4   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "\n",
       "   c.newinc_y  c.notified_y    country_y  iso.numeric_y  c.new.tsr_y  \\\n",
       "0      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "1      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "2      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "3      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "4      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "\n",
       "   Latitude  Longitude  \n",
       "0  12.52111 -69.968338  \n",
       "1  12.52111 -69.968338  \n",
       "2  12.52111 -69.968338  \n",
       "3  12.52111 -69.968338  \n",
       "4  12.52111 -69.968338  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file to inspect its contents\n",
    "file_path = 'Merged_Data_with_Latitude_and_Longitude.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the data to understand its structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dc88b336-64c6-405a-b687-d697623faf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hierarchical_tree_map.json'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Group data by 'g.whoregion_x' and 'country_x' to create hierarchical structure\n",
    "hierarchy_data = data.groupby(['g.whoregion_x', 'country_x', 'year'])[['new.ep']].sum().reset_index()\n",
    "\n",
    "# Creating the hierarchical structure\n",
    "def create_hierarchy(data):\n",
    "    regions = data['g.whoregion_x'].unique()\n",
    "    hierarchy = {'name': 'World', 'children': []}\n",
    "\n",
    "    for region in regions:\n",
    "        region_data = data[data['g.whoregion_x'] == region]\n",
    "        region_node = {'name': region, 'children': []}\n",
    "\n",
    "        countries = region_data['country_x'].unique()\n",
    "        for country in countries:\n",
    "            country_data = region_data[region_data['country_x'] == country]\n",
    "            country_node = {'name': country, 'newinc_tb': country_data['new.ep'].sum(), 'children': []}\n",
    "\n",
    "            for year in country_data['year'].unique():\n",
    "                year_data = country_data[country_data['year'] == year]\n",
    "                country_node['children'].append({\n",
    "                    'name': str(year),\n",
    "                    'newinc_tb': year_data['new.ep'].sum()\n",
    "                })\n",
    "\n",
    "            region_node['children'].append(country_node)\n",
    "\n",
    "        hierarchy['children'].append(region_node)\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "# Create the hierarchy structure\n",
    "hierarchy = create_hierarchy(hierarchy_data)\n",
    "\n",
    "# Write the hierarchy to a JSON file\n",
    "output_path = 'hierarchical_tree_map.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(hierarchy, json_file, indent=2)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "30653eca-8a24-44a3-9450-0be037a3794f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>iso3</th>\n",
       "      <th>year</th>\n",
       "      <th>country_x</th>\n",
       "      <th>iso2_x</th>\n",
       "      <th>iso.numeric_x</th>\n",
       "      <th>g.whoregion_x</th>\n",
       "      <th>new.ep</th>\n",
       "      <th>tot.newrel</th>\n",
       "      <th>c.newunk</th>\n",
       "      <th>...</th>\n",
       "      <th>conf</th>\n",
       "      <th>ep</th>\n",
       "      <th>g.whoregion_y</th>\n",
       "      <th>c.newinc_y</th>\n",
       "      <th>c.notified_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>iso.numeric_y</th>\n",
       "      <th>c.new.tsr_y</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1980</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1981</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1982</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1983</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ABW</td>\n",
       "      <td>1984</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AW</td>\n",
       "      <td>533.0</td>\n",
       "      <td>AMR</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2118.5</td>\n",
       "      <td>2231.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702607</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>2535.5</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>430.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.52111</td>\n",
       "      <td>-69.968338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 iso3  year country_x iso2_x  iso.numeric_x g.whoregion_x  \\\n",
       "0           0  ABW  1980     Aruba     AW          533.0           AMR   \n",
       "1           1  ABW  1981     Aruba     AW          533.0           AMR   \n",
       "2           2  ABW  1982     Aruba     AW          533.0           AMR   \n",
       "3           3  ABW  1983     Aruba     AW          533.0           AMR   \n",
       "4           4  ABW  1984     Aruba     AW          533.0           AMR   \n",
       "\n",
       "   new.ep  tot.newrel  c.newunk  ...      conf        ep  g.whoregion_y  \\\n",
       "0   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "1   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "2   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "3   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "4   365.0      2118.5    2231.5  ...  0.702607  0.090036            EUR   \n",
       "\n",
       "   c.newinc_y  c.notified_y    country_y  iso.numeric_y  c.new.tsr_y  \\\n",
       "0      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "1      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "2      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "3      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "4      2448.0        2535.5  Afghanistan          430.0         81.0   \n",
       "\n",
       "   Latitude  Longitude  \n",
       "0  12.52111 -69.968338  \n",
       "1  12.52111 -69.968338  \n",
       "2  12.52111 -69.968338  \n",
       "3  12.52111 -69.968338  \n",
       "4  12.52111 -69.968338  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file to examine its structure\n",
    "file_path = 'Merged_Data_with_Latitude_and_Longitude.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a515e6fa-0df6-4881-bb7e-ac440f78a4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunburst_chart_data.json'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare the data for sunburst chart\n",
    "sunburst_data = {\"name\": \"World\", \"children\": []}\n",
    "\n",
    "# Group the data by region\n",
    "regions = data['g.whoregion_x'].unique()\n",
    "for region in regions:\n",
    "    region_data = {\n",
    "        \"name\": region,\n",
    "        \"children\": []\n",
    "    }\n",
    "\n",
    "    # Group by country within the region\n",
    "    countries = data[data['g.whoregion_x'] == region]['country_x'].unique()\n",
    "    for country in countries:\n",
    "        country_data = {\n",
    "            \"name\": country,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        # Group by year within the country\n",
    "        years = data[data['country_x'] == country]['year'].unique()\n",
    "        for year in years:\n",
    "            year_data = {\n",
    "                \"name\": str(year),\n",
    "                \"newinc_tb\": data[(data['country_x'] == country) & (data['year'] == year)]['new.ep'].sum()\n",
    "            }\n",
    "            country_data['children'].append(year_data)\n",
    "\n",
    "        region_data['children'].append(country_data)\n",
    "\n",
    "    # Add region data to the root\n",
    "    sunburst_data['children'].append(region_data)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "output_json_path = 'sunburst_chart_data.json'\n",
    "with open(output_json_path, 'w') as f:\n",
    "    json.dump(sunburst_data, f, indent=2)\n",
    "\n",
    "output_json_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc75a4-428a-4d61-bb01-ddc4fa01d529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
